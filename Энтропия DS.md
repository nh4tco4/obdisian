#ds 

Энтропия - мера беспорядка. Принимает значения от 0 до 1. В зависимости от количества классов в нашем наборе данных значение энтропии может оказаться больше 1, но означать это будет все то же, что уровень беспорядка крайне высок. Математическая формула:
$$
H(A) = - \sum_{i=1}^m P_{i} \log_{2} P_{i}
$$
$P_{i}$ - частотная вероятность каждого $i$ класса данных.

Пусть есть три класса данных:

| Класс   | Вероятность   | Значение |
| ------- | ------------- | -------- |
| $p_{1}$ | $\frac{2}{5}$ | 0        |
| $p_2$   | $\frac{1}{5}$ | 1        |
| $p_3$   | $\frac{3}{5}$ | 2        |

Тогда посчитаем энтропию:
$$
H(A) = - \frac{2}{5} \cdot \log_{2} \frac{2}{5} - \frac{1}{5} \cdot \log_{2} \frac{1}{5} - \frac{3}{5} \cdot \log_{2} \frac{3}{5} = 1.435
$$

https://habr.com/ru/companies/otus/articles/502200/